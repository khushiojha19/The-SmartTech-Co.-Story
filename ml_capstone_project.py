# -*- coding: utf-8 -*-
"""ML_Capstone_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11jfNvr15RLDEqMT5Jp18K0wOdaNuJm-5

**Machine Learning Capstone Project**

**Step 01 : Uploading the dataset**
"""

from google.colab import files


# Upload the file

uploaded = files.upload()


# Importing the library 'Pandas' to load and read the file

import pandas as pd

data = pd.read_csv("laptop.csv")

"""**Step 02 : Inspecting the Dataset**"""

# To check the first 5 rows of the dataset

print(data.head())

# To get a structural overview of the dataset

print(data.info())

# To get statistical summary of the dataset

print(data.describe())

"""**Step 03 : Data Cleaning**"""

# Check for missing values

missing_values = data.isnull().sum()

print("Missing Values: \n" , missing_values)

# Drop unnecessary columns (e.g., Unnamed: 0)

data = data.drop(columns=['Unnamed: 0'])

# Handle missing values

data = data.dropna()

"""**Step 04 : Data Pre-Processing**"""

# Encoding categorical features by importing the class' LabelEncoder'

from sklearn.preprocessing import LabelEncoder

categorical_cols = ['Company', 'TypeName', 'OpSys', 'Gpu', 'Cpu']

for col in categorical_cols:
    data[col] = LabelEncoder().fit_transform(data[col])

# Converting 'Weight' column to numeric

#Ensure the 'Weight' column is treated as a string

data['Weight'] = data['Weight'].astype(str)


# Replace '?' with 0 before cleaning

data['Weight'] = data['Weight'].replace('?', float('0'))


# Remove any leading/trailing spaces and 'kg', then convert to float

data['Weight'] = data['Weight'].str.strip().str.replace('kg', '').astype(float)


# Handle NaN values by filling them with the mean

data['Weight'] = data['Weight'].fillna(data['Weight'].mean())


# Print the first few rows to verify the result

print(data['Weight'].head())

# Standardize numerical columns by importing class ' StandardScalar'

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()


# Clean 'Inches' column

data['Inches'] = data['Inches'].astype(str)                       # Convert to string if not already
data['Inches'] = data['Inches'].replace('?', float('0'))          # Replace '?' with 0
data['Inches'] = data['Inches'].str.strip().astype(float)         # Convert to float (after cleaning)


# Define the numerical columns for scaling

num_cols = ['Inches', 'Weight']

# Apply standardization

data[num_cols] = scaler.fit_transform(data[num_cols])

# Step 5: Print first few rows to check the result

print(data[num_cols].head())

"""**Step 05 : Exploratory Data Analysis**"""

# Importing the 'Seaborn' Library and 'pyplot' class from 'Matplotlib' Library

import seaborn as sns
import matplotlib.pyplot as plt

# Correlation heatmap

# Creates a figure of size 10 x 6 inches

plt.figure(figsize=(10, 6))


# Select only numerical columns from the dataset

numeric_data = data.select_dtypes(include=['float64', 'int64'])


# Creates a heatmap to visualize correlation matrix

sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')


plt.title("Feature Correlations with Price")

plt.show()

# Distribution of Price

# Creates histogram to visualize distribution of single variable

sns.histplot(data['Price'], kde=True, bins=30)


plt.title("Price Distribution")

plt.show()

"""**Step 06 : Feature Engineering**"""

print(data.columns)

"""**Step 07 : Model Development**"""

# Importing the 'Train Test Split' function from the 'Model Selection' module of 'sklearn' Library to help split your data into training and testing sets

from sklearn.model_selection import train_test_split


# Importing the 'Linear Regression' class from the 'Linear Model' module of 'sklearn' Library to create a linear regression model

from sklearn.linear_model import LinearRegression


# Importing the 'Random Forest Regressor' class from the 'Ensemble' module of 'sklearn' Library to create a random forest regression model

from sklearn.ensemble import RandomForestRegressor


# Importing the 'Mean Squared Error (MSE)' and 'R2 Score' functions from the 'Metrics' module of 'sklearn' Library to evaluate the performance of models

from sklearn.metrics import mean_squared_error, r2_score

# Splitting the data into 'x' and 'y' features


X = data.drop(columns=['Price'])

y = data['Price']


# Apply one-hot encoding to categorical features

X = pd.get_dummies(X, drop_first=True)     # Drop the first category

# Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression Model

from sklearn.impute import SimpleImputer


# Handle missing values by imputation (using the mean strategy)

imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)


# Initialize the Linear Regression model
lr = LinearRegression()


# Fit the model to the training data
lr.fit(X_train, y_train)


# Make predictions on the test set
lr_preds = lr.predict(X_test)

# Random Forest Regressor Model


# Initialize the Random Forest Regressor model
rf = RandomForestRegressor()

# FIt the model to the training data
rf.fit(X_train, y_train)

# Make predictions on the test set
rf_preds = rf.predict(X_test)

"""**Step 08 : Evaluating the performances of the Model**"""

# Evaluating the performance of Linear Regression Model

import numpy as np


print('Linear Regression:')

print('RMSE:', np.sqrt(mean_squared_error(y_test, lr_preds)))

print('R2 Score:', r2_score(y_test, lr_preds))

# Evaluating the performance of Random Forest Regressor Model

print('Random Forest Regressor:')

print('RMSE:', np.sqrt(mean_squared_error(y_test, rf_preds)))

print('R2 Score:', r2_score(y_test, rf_preds))

"""**Step 09 : Comparing the Models**"""

model_comparison = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest Regressor'],

    'RMSE': [np.sqrt(mean_squared_error(y_test, lr_preds)),
              np.sqrt(mean_squared_error(y_test, rf_preds))],

    'R2 Score': [
        r2_score(y_test, lr_preds),
        r2_score(y_test, rf_preds)]
})

print(model_comparison)

"""**Step 10 : Hyperparamter Tuning**"""

from sklearn.model_selection import GridSearchCV

# Defining the Hyperparamter Grid

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20]
}


# Setting up Grid Search CV

grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)


# Fitting the Model

grid_search.fit(X_train, y_train)


# Selecting the Best Model

best_rf = grid_search.best_estimator_


# Making predictions and evaluating the Tuned Model

best_rf_preds = best_rf.predict(X_test)
mse = mean_squared_error(y_test, best_rf_preds)
rmse = np.sqrt(mse)
print("Tuned Random Forest RMSE:", rmse)